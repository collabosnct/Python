{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**ğŸ¯èª²é¡Œã®ç›®çš„**"
      ],
      "metadata": {
        "id": "WYUaTSri-sm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã«ãªã‚ŠãŒã¡ãªAIã®å­¦ç¿’ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã€ç‰¹ã« **ã€Œèª¤å·®é€†ä¼æ’­æ³• (Backpropagation)ã€** ã‚’ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒª (PyTorch/TensorFlowãªã©) ã‚’ä½¿ã‚ãšã«ã€**NumPyï¼ˆè¡Œåˆ—è¨ˆç®—ï¼‰ã®ã¿**ã§å®Ÿè£…ã—ã¾ã™ã€‚\n",
        "\n",
        "ã“ã‚ŒãŒæ›¸ã‘ã‚Œã°ã€ã‚ãªãŸã¯ã€ŒAIã®ä¸­èº«ã€ã‚’çœŸã«ç†è§£ã—ãŸã¨è¨€ãˆã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "EUD7_ywT-1aP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ğŸ–Šæº–å‚™**"
      ],
      "metadata": {
        "id": "elglQ_hZAWER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **ç’°å¢ƒï¼š** Google Colaboratoryã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ã®Pythonã®ç’°å¢ƒ\n",
        "*   **å¿…é ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª**ï¼š`numpy`, `sklearn`, `matplotlib`\n",
        "\n"
      ],
      "metadata": {
        "id": "gOYdv1ZfAdlv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ã€èª²é¡Œï¼‘ã€‘2å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å®Ÿè£…ï¼ˆç©´åŸ‹ã‚ï¼‰**"
      ],
      "metadata": {
        "id": "IeRB_hPxBMBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ã€æ‰‹æ›¸ãæ•°å­—ï¼ˆ8Ã—8ãƒ”ã‚¯ã‚»ãƒ«ï¼‰ã‚’èªè­˜ã™ã‚‹AIã®ã‚¯ãƒ©ã‚¹ã§ã™ã€‚`gradient`ãƒ¡ã‚½ãƒƒãƒ‰å†…ã®`#[TODO]`ã¨æ›¸ã‹ã‚ŒãŸéƒ¨åˆ†ï¼ˆé€†ä¼æ’­ã®è¨ˆç®—ï¼‰ã‚’åŸ‹ã‚ã¦ã€ã‚¯ãƒ©ã‚¹ã‚’å®Œæˆã•ã›ã¦ãã ã•ã„ã€‚"
      ],
      "metadata": {
        "id": "hBn3wibXBiU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ãƒ’ãƒ³ãƒˆï¼šé€†ä¼æ’­ã®æ•°å¼ï¼ˆè¡Œåˆ—ã®å½¢çŠ¶ã«æ³¨æ„ï¼ï¼‰**\n",
        "\n",
        "\n",
        "\n",
        "1.   **å‡ºåŠ›å±¤ã®èª¤å·®**ï¼š$\\frac{âˆ‚L}{âˆ‚y}$ = y - t (Softmax + CrossEntropyã®å ´åˆ)\n",
        "2.   **ç¬¬2å±¤ã®é‡ã¿å‹¾é…**ï¼š$\\frac{âˆ‚L}{âˆ‚W2}$ = $z_1^T$ãƒ»(y - t)\n",
        "3.   **éš ã‚Œå±¤ã¸ã®é€†ä¼æ’­**ï¼š$Î´_1$ = (y - t)ãƒ»$W2^T$ Ã— sigmoid'($a_1$)\n"
      ],
      "metadata": {
        "id": "FHq9qep_Camx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyHdc_H6-BAU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "#â€â€â€æ´»æ€§åŒ–é–¢æ•°ã¨å¾®åˆ†ã®å®šç¾©â€â€â€\n",
        "def sigmoid\n",
        "    return\n",
        "\n",
        "def sigmoid_grad\n",
        "    return\n",
        "\n",
        "def softmax\n",
        "    x =  #ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å¯¾ç­–\n",
        "    return np.exp\n",
        "\n",
        "def cross_entropy_error\n",
        "    if y.ndim ==\n",
        "        t =\n",
        "        y =\n",
        "    if t.size ==\n",
        "        t =\n",
        "    batch_size =\n",
        "    return\n",
        "\n",
        "#â€â€â€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¯ãƒ©ã‚¹ï¼ˆèª²é¡Œå¯¾è±¡ï¼‰â€â€â€\n",
        "class TwoLayerNet:\n",
        "    def__init__\n",
        "\n",
        "        #é‡ã¿ã®åˆæœŸåŒ–\n",
        "        self.params =\n",
        "        self.params['W1'] =\n",
        "        self.params['b1'] =\n",
        "        self.params['W2'] =\n",
        "        self.params['b2'] =\n",
        "\n",
        "    def predict\n",
        "        #é †ä¼æ’­ (Forward)\n",
        "        W1, W2 =\n",
        "        b1, b2 =\n",
        "\n",
        "        a1 = np.dot\n",
        "        z1 = sigmoid\n",
        "        a2 = np.dot\n",
        "        y = softmax\n",
        "        return\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict\n",
        "        return\n",
        "\n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict\n",
        "        y = np.argmax\n",
        "        t = np.argmax\n",
        "        return\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        #èª¤å·®é€†ä¼æ’­æ³• (Backward)\n",
        "        W1, W2 =\n",
        "        b1, b2 =\n",
        "        grads =\n",
        "        batch_size =\n",
        "\n",
        "        #1. Forward ï¼ˆè¨ˆç®—ã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰ï¼‰\n",
        "        a1 = np.dot\n",
        "        z1 = sigmoid\n",
        "        a2 = np.dot\n",
        "        y = softmax\n",
        "\n",
        "        #2. Backwardï¼ˆã“ã“ã‚’å®Ÿè£…ï¼ï¼‰\n",
        "\n",
        "        #Step A: å‡ºåŠ›å±¤ã®èª¤å·® (y - t)\n",
        "        #æ•™å¸«ãƒ‡ãƒ¼ã‚¿ t ã¨äºˆæ¸¬ y ã®å·®åˆ†ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„\n",
        "        #æå¤±é–¢æ•°ã®å¾®åˆ† (Softmax + CrossEntropy)\n",
        "        dy =\n",
        "\n",
        "        #Step B: ç¬¬2å±¤ï¼ˆå‡ºåŠ›å±¤ï¼‰ã®å‹¾é…\n",
        "        #W2ã®å‹¾é… =ï¼ˆéš ã‚Œå±¤å‡ºåŠ› z1ï¼‰^Tãƒ»dy\n",
        "        grads['W2'] =\n",
        "        #b2ã®å‹¾é… = dyã®ç·å’Œ (axis=0)\n",
        "        grads['b2'] =\n",
        "\n",
        "        #Step C: éš ã‚Œå±¤ã¸ã®é€†ä¼æ’­\n",
        "        #å‡ºåŠ›å±¤ã‹ã‚‰ã®èª¤å·® dy ã«W2ã®è»¢ç½®è¡Œåˆ—ã‚’æ›ã‘ã‚‹\n",
        "        dz1 = np.dot\n",
        "\n",
        "        #Step D: æ´»æ€§åŒ–é–¢æ•°(Sigmoid)ã®å£ã‚’é€šã™\n",
        "        #sigmoid_grad(a1)ã‚’æ›ã‘ã‚‹ï¼ˆè¦ç´ ã”ã¨ã®ç©ï¼‰\n",
        "        da1 = sigmoid_grad\n",
        "\n",
        "        #Step E: ç¬¬1å±¤ï¼ˆéš ã‚Œå±¤ï¼‰ã®å‹¾é…\n",
        "        #W1ã®å‹¾é… = å…¥åŠ› x^Tãƒ»da1\n",
        "        grads['W1'] =\n",
        "        #b1ã®å‹¾é… = da1ã®ç·å’Œ\n",
        "        grads['b1'] =\n",
        "\n",
        "        return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ã€èª²é¡Œï¼’ã€‘å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œã¨ç¢ºèª**"
      ],
      "metadata": {
        "id": "ATazZ4eSYfA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "èª²é¡Œï¼‘ã§å®Ÿè£…ã—ãŸã‚¯ãƒ©ã‚¹ã‚’ä½¿ã£ã¦ã€å®Ÿéš›ã«AIã‚’å­¦ç¿’ã•ã›ã¾ã™ã€‚ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã€çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
      ],
      "metadata": {
        "id": "j9iguQBGYl0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "#ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰\n",
        "digits = load_digits()\n",
        "x = digits.data\n",
        "y = digits.target\n",
        "\n",
        "#æ­£è¦åŒ– (0-16 â†’ 0.0-1.0)\n",
        "x = x / 16.0\n",
        "\n",
        "#One-Hot Encoding (ä¾‹ï¼š2 â†’ [0,0,1,0,...])\n",
        "enc = OneHotEncoder()\n",
        "y_onehot = enc.fit_transform(y.reshape(-1, 1)).toarray()\n",
        "\n",
        "#åˆ†å‰²\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y_onehot, test_size=0.2)\n",
        "\n",
        "#â€â€â€å­¦ç¿’å®Ÿè¡Œéƒ¨â€â€â€\n",
        "network = TwoLayerNet(input_size=64, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 1000\n",
        "learning_rate = 0.1\n",
        "train_loss_list = []\n",
        "acc_list =[]\n",
        "\n",
        "print(\"å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "\n",
        "for i in range(iters_num):\n",
        "    #ãƒŸãƒ‹ãƒãƒƒãƒå–å¾—\n",
        "    batch_mask = np.random.choice(x_train.shape[0], 100)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = y_train[batch_mask]\n",
        "\n",
        "    #å‹¾é…è¨ˆç®—ï¼ˆã‚ãªãŸã®å®Ÿè£…ã—ãŸgradientãŒå‘¼ã°ã‚Œã¾ã™ï¼‰\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "    #ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    #è¨˜éŒ²\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        acc = network.accuracy(x_test, y_test)\n",
        "        acc_list.append(acc)\n",
        "        print(f\"Iter {i}: Loss {loss:.4f}, Accuracy {acc:.3f}\")\n",
        "#æœ€çµ‚çµæœ\n",
        "final_acc = network.accuracy(x_test, y_test)\n",
        "print(f\"æœ€çµ‚ãƒ†ã‚¹ãƒˆç²¾åº¦: {final_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "oF1ad9dATlya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**âœ…æˆåŠŸã®åŸºæº–**\n",
        "\n",
        "\n",
        "*   ã‚¨ãƒ©ãƒ¼ãŒå‡ºãšã«å®Ÿè¡Œã§ãã‚‹ã“ã¨ã€‚\n",
        "*   `Loss`ï¼ˆèª¤å·®ï¼‰ãŒå¾ã€…ã«æ¸›ã£ã¦ã„ã‚‹ã“ã¨ï¼ˆä¾‹ï¼š2.3â†’1.5â†’0.8...ï¼‰ã€‚\n",
        "*   æœ€çµ‚çš„ãª`Accuracy`ï¼ˆæ­£è§£ç‡ï¼‰ãŒ**90%ä»¥ä¸Š**ã«ãªã‚‹ã“ã¨ã€‚\n",
        "\n"
      ],
      "metadata": {
        "id": "fgsh4glph75_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ã€èª²é¡Œï¼“ã€‘ãƒãƒ£ãƒ¬ãƒ³ã‚¸å•é¡Œï¼ˆè€ƒå¯Ÿï¼‰**"
      ],
      "metadata": {
        "id": "QZuRQ0yWi37P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã‚³ãƒ¼ãƒ‰ãŒå‹•ã„ãŸã‚‰ã€ä»¥ä¸‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰æ›´ã—ã¦æŒ™å‹•ãŒã©ã†å¤‰ã‚ã‚‹ã‹å®Ÿé¨“ã—ã€ç°¡å˜ãªãƒ¬ãƒãƒ¼ãƒˆï¼ˆ1-2è¡Œï¼‰ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "\n",
        "1.   **å­¦ç¿’ç‡**ï¼ˆ`learning_rate`ï¼‰ã‚’æ¥µç«¯ã«å¤§ããï¼ˆä¾‹ï¼š`10.0`ï¼‰ã€ã¾ãŸã¯å°ã•ãï¼ˆä¾‹ï¼š`0.0001`ï¼‰ã™ã‚‹ã¨ã©ã†ãªã‚Šã¾ã™ã‹ï¼Ÿ\n",
        "2.   **éš ã‚Œå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°**ï¼ˆ`hidden_size`ï¼‰ã‚’`50`ã‹ã‚‰`5`ã«æ¸›ã‚‰ã™ã¨ã€ç²¾åº¦ã¯ã©ã†å¤‰åŒ–ã—ã¾ã™ã‹ï¼Ÿãã‚Œã¯ãªãœã ã¨æ€ã„ã¾ã™ã‹ï¼Ÿ\n"
      ],
      "metadata": {
        "id": "6hjGZkhOjAfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ï¼ˆå›ç­”æ¬„ï¼‰\n",
        "\n",
        "\n",
        "1.   \n",
        "2.   \n",
        "\n"
      ],
      "metadata": {
        "id": "ovKKl8HLk14_"
      }
    }
  ]
}