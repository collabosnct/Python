{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "🎯**目的**"
      ],
      "metadata": {
        "id": "SQNakMiu8hg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "前回は「公式」を使って一発で答えを出しましたが、複雑なニューラルネットワークではそのような公式が存在しません。そこで、 **「とりあえず適当な値からスタートし、少しずつ誤差が減る方向へ修正していく」** というアプローチをとります。これが **勾配降下法(Gradient Descent)** です。\n",
        "\n",
        "今回は、前回と同じデータに対して、この「反復学習」を行い、AIが徐々に賢くなっていく様子を観察します。"
      ],
      "metadata": {
        "id": "qQAoW4RE8-oX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🖊**前提知識**"
      ],
      "metadata": {
        "id": "kZ9SCpZIBq2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **学習率(Learning Rate, η)**：一回の修正でどれくらい値を動かすか（歩幅）。\n",
        "*   **勾配(Gradient)**：パラメータを増やした時、誤差が増えるか減るかを表す傾き。\n",
        "*   **更新式**：W<sub>new</sub> = W<sub>old</sub> - η × 勾配（傾きと逆方向に進むことで谷底へ降りる）\n"
      ],
      "metadata": {
        "id": "VtKcQg7uCJdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**【準備】データの生成**"
      ],
      "metadata": {
        "id": "_OD6-9WyD4BM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "前回と同じデータセットを作ります。"
      ],
      "metadata": {
        "id": "P7gl6J34EKko"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrDPg78C71gZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#ランダムシード固定\n",
        "\n",
        "\n",
        "#データ生成\n",
        "\n",
        "\n",
        "#予測関数（前回作成したもの）\n",
        "\n",
        "\n",
        "#誤差関数（MSE）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**課題１：勾配（微分）の計算**"
      ],
      "metadata": {
        "id": "PKwrvUecGegB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "「今の a と b だと、どっちに動かせば誤差が減るか？」を計算する関数 `compute_gradients(x, y, a, b)` を作ります。\n",
        "\n",
        "誤差関数 L を a と b で偏微分すると、以下の式になります（数式の導出は一旦置いておき、これをコードにします。）"
      ],
      "metadata": {
        "id": "k9oFZ9PjGna8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "$$ \\frac{\\partial L}{\\partial a} = \\frac{2}{N}\\sum(y_{pred} - y_{true})\\cdot x $$ $$\\frac{\\partial L}{\\partial b} = \\frac{2}{N}\\sum(y_{pred} - y_{true})$$"
      ],
      "metadata": {
        "id": "BwdhOg3fIcHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*ヒント：`y_pred` を計算した後、平均 `np.mean()` を使うと `1/N * sum(...)` の部分が計算できます。係数の `2` を忘れずに。*"
      ],
      "metadata": {
        "id": "UhMG00D6JaYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#【課題１】勾配を計算する関数\n",
        "\n",
        "    #1. 現在の予測値を計算\n",
        "\n",
        "\n",
        "    #2. 誤差（予測 - 正解）を計算\n",
        "\n",
        "\n",
        "    #3. aの勾配（誤差 × 入力x の平均 × 2）\n",
        "\n",
        "\n",
        "    #4. bの勾配（誤差の平均 × 2）\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "keCcLiWuJ4No"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**課題２：学習ループの実装（メイン課題）**"
      ],
      "metadata": {
        "id": "_ghFTFLSQxvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "いよいよ学習です！以下の手順でパラメータを更新するループを書いてください。\n",
        "\n",
        "\n",
        "1. パラメータ a,b をランダムに初期化する。\n",
        "2. 以下を **1000回(epochs)** 繰り返す：\n",
        "\n",
        "   ・現在の a,b で勾配を計算する。\n",
        "\n",
        "   ・パラメータを更新する：a = a - 学習率 × 勾配\n",
        "\n",
        "   ・100回ごとに現在の誤差(MSE)を表示して、学習の進み具合を確認する。"
      ],
      "metadata": {
        "id": "opXWU2BnQ6VM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**設定値：**\n",
        "*   学習率 (`learning_rate`) = 0.01\n",
        "*   繰り返し回数 (`epochs`) = 1000\n",
        "\n"
      ],
      "metadata": {
        "id": "Igkvf8wZBVQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#【課題２】学習ループの実装\n",
        "#初期化（適当な値からスタート）\n",
        "\n",
        "\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "#誤差の履歴を保存するリスト（グラフ描画用）\n",
        "\n",
        "\n",
        "\n",
        "    #1. 勾配を計算（課題１の関数を使用）\n",
        "\n",
        "\n",
        "    #2. パラメータの更新（学習率 * 勾配 を引く）\n",
        "\n",
        "\n",
        "    #記録\n",
        "    #現在の誤差を計算して保存\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "H7J0iOhQLa74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**課題３：学習曲線の可視化**"
      ],
      "metadata": {
        "id": "fL-9Y2RXI-cj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AIが賢くなっていく過程（誤差が減っていく様子）をグラフにしましょう。`loss_history` をプロットしてください。これが「右肩下がりのグラフ」になれば学習成功です。"
      ],
      "metadata": {
        "id": "-H7NeRr5JI5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#【課題３】学習曲線 (Learning Curve)の描画\n",
        "#横軸：繰り返し回数，縦軸：誤差 (MSE)\n",
        "\n",
        "\n",
        "\n",
        "#解説：最初は急激に誤差が減り、徐々に平らになっていく（収束する）様子が確認できます。"
      ],
      "metadata": {
        "id": "1PE9gDjyFO7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**課題４：結果の確認**"
      ],
      "metadata": {
        "id": "8RE_k9egPg6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "最後に、学習した a,b で回帰直線をデータに重ねて表示しましょう。（前回の課題５と同じ要領です）"
      ],
      "metadata": {
        "id": "eRCJfF0ePmuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#【課題４】データ点と回帰直線の描画\n",
        "#散布図\n",
        "\n",
        "\n",
        "#回帰直線（x_lineは0～10）\n",
        "\n",
        "\n",
        "\n",
        "#解説：前回の「最小二乗法」の結果とほぼ同じ直線が引けていれば成功です！"
      ],
      "metadata": {
        "id": "qXQ04hmJLzFP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}